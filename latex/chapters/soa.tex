\chapter{State of the Art}
\section{IDQN}\label{sec:IDQN}
Reinforcement Learning (RL) is a prominent area of machine learning where agents learn to make sequential decisions by interacting with an environment. DQN, short for Deep Q-Network, is a fundamental algorithm in RL that leverages deep neural networks to approximate optimal action-value functions.

\subsection{Deep Q-Network (DQN)}

DQN, proposed by Mnih et al. \cite{mnih2015human}, is designed to address the challenges of learning Q-values in high-dimensional state spaces. It combines Q-learning, a well-established RL algorithm, with deep neural networks.

The Q-value, denoted as \(Q(s, a)\), represents the expected cumulative reward when taking action \(a\) in state \(s\). DQN approximates this Q-value using a deep neural network with parameters \(\theta\). The Q-network is trained to minimize the temporal difference (TD) error:

\[
\delta = Q(s, a;\theta) - (r + \gamma \max_{a'} Q(s', a';\theta^-))
\]

Where:
\begin{align*}
    &\delta \text{ - TD error}\\
    &Q(s, a;\theta) \text{ - Q-value predicted by the network}\\
    &r \text{ - Immediate reward}\\
    &\gamma \text{ - Discount factor}\\
    &Q(s', a';\theta^-) \text{ - Target Q-value predicted by a target network with parameters }\theta^-
\end{align*}

DQN employs experience replay and a target network to stabilize training. Experience replay stores past experiences in a replay buffer and samples mini-batches for training, breaking the temporal correlation in the data. The target network provides stable target Q-values for the TD error.

\subsection{Independent Deep Q-Networks (IDQN)}

IDQN is an extension of DQN tailored for multi-agent RL scenarios, where multiple agents operate independently to optimize their actions. Each agent in IDQN maintains its own Q-network and replay buffer.

The Q-value update rule in IDQN remains similar to DQN, but it is extended to accommodate multiple agents:

\[
\delta = Q_i(s, a_i;\theta_i) - (r + \gamma \max_{a'} Q_i(s', a';\theta^-))
\]

Where:
\begin{align*}
    &\delta \text{ - TD error for agent } i\\
    &Q_i(s, a_i;\theta_i) \text{ - Q-value predicted by agent } i's \text{ network}\\
    &r \text{ - Immediate reward}\\
    &\gamma \text{ - Discount factor}\\
    &Q_i(s', a';\theta^-) \text{ - Target Q-value predicted by agent } i's \text{ target network}
\end{align*}

IDQN facilitates decentralized decision-making among multiple agents, making it suitable for scenarios involving cooperation or competition among agents.


To explore IDQN in more detail, the following paper\cite{ault2020learning} provide comprehensive insights into its theory and applications

\section{IPPO}\label{sec:IPPO}
Proximal Policy Optimization (PPO) is a state-of-the-art reinforcement learning algorithm designed for optimizing parameterized policies in complex environments. IPPO, short for Independent Proximal Policy Optimization, is an extension of PPO tailored for multi-agent reinforcement learning scenarios, where multiple agents learn independently.

\subsection{Proximal Policy Optimization (PPO)}

Introduced by Schulman et al. \cite{schulman2017proximal}, PPO addresses several challenges in policy optimization. It aims to maximize the expected cumulative reward while ensuring that policy updates are not too large, preventing catastrophic policy changes. PPO achieves this through the following objectives:

\subsubsection{Objective Function}

PPO optimizes a surrogate objective function that balances the trade-off between policy improvement and policy constraint. The objective function is given as:

\[
\mathcal{L}(\theta) = \mathbb{E}\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}\left(r_t(\theta), 1-\epsilon, 1+\epsilon\right)\hat{A}_t\right)\right]
\]

Where:
\begin{align*}
    &\mathcal{L}(\theta) \text{ - Surrogate objective function}\\
    &\theta \text{ - Policy parameters}\\
    &r_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} \text{ - Importance ratio}\\
    &\hat{A}_t \text{ - Advantage estimate}\\
    &\epsilon \text{ - Clip parameter}
\end{align*}

PPO optimizes this objective function using stochastic gradient ascent.

\subsubsection{Trust Region}

PPO introduces a trust region constraint by clipping the surrogate objective. The clip function ensures that policy updates do not deviate significantly from the previous policy:

\[
\text{clip}(x, a, b) = \begin{cases}
    x, & \text{if } x \in [a, b]\\
    a, & \text{if } x < a\\
    b, & \text{if } x > b
\end{cases}
\]

PPO efficiently balances policy updates to ensure stability and improved performance.

\subsection{Independent Proximal Policy Optimization (IPPO)}

IPPO extends the PPO algorithm for multi-agent RL scenarios, where multiple agents learn independently. Each agent in IPPO maintains its own policy and operates in the environment. IPPO's objective function for agent \(i\) remains similar to PPO:

\[
\mathcal{L}_i(\theta_i) = \mathbb{E}\left[\min\left(r_t(\theta_i)\hat{A}_t^i, \text{clip}\left(r_t(\theta_i), 1-\epsilon, 1+\epsilon\right)\hat{A}_t^i\right)\right]
\]

Where:
\begin{align*}
    &\mathcal{L}_i(\theta_i) \text{ - Surrogate objective function for agent } i\\
    &\theta_i \text{ - Policy parameters for agent } i\\
    &r_t(\theta_i) = \frac{\pi_{\theta_i}(a_t^i|s_t)}{\pi_{\theta_{i_{\text{old}}}}(a_t^i|s_t)} \text{ - Importance ratio for agent } i\\
    &\hat{A}_t^i \text{ - Advantage estimate for agent } i
\end{align*}

IPPO facilitates decentralized learning among multiple agents, making it suitable for scenarios involving independent agents with their policies.


To explore IPPO in more detail, the following paper\cite{ault2020learning} provide comprehensive insights into its theory and applications


\section{MPLight}\label{sec:MPLight}
The MPLight\cite{chen2020toward} traffic light control system is one that makes effective use of the idea of pressure in order to organize the flow of traffic through many intersections. In order for it to function properly, it takes into account the pressure, which can be defined as the disparity between the lengths of the lines waiting to enter a junction and the line waiting to enter an intersection farther downstream. MPLight is intended to improve the flow of traffic in metropolitan areas while simultaneously lowering levels of congestion.

Within MPLight, pressure is utilized as an essential parameter for the purpose of coordinating traffic signals. It is determined by subtracting the length of the line formed by cars waiting to enter a junction from the length of the line formed by vehicles waiting to enter the receiving lane of the intersection farther downstream. MPLight seeks to achieve a balance in the traffic load at numerous junctions by taking pressure into consideration.

MPLight is a method for controlling traffic lights that was developed by Chen et al., and it makes use of principles from reinforcement learning. In order to arrive at judgments about the traffic signals, they made use of Deep Q-Networks (DQN) as the underlying architecture. One DQN agent serves as the point of contact for all of the intersections in this configuration.

In MPLight, pressure is employed not only as a coordination metric but also as the state and the reward for the DQN agent. This is because pressure is a measure of how well the agents are working together. At every particular time step, information about the pressure values at all relevant intersections is included in the state of the agent that is being described. The DQN agent's capacity to learn is facilitated by the reward signal, which is derived from changes in pressure and serves as a guide for that process.

When compared to the approaches that are currently being used, Chen et al.\cite{chen2020toward} showed substantial improvements in both the flow of traffic and the journey times when MPLight was used. To be more specific, MPLight was able to produce up to a 19.2 percent improvement in travel times when compared to the next best approach, which was PressLight.


\section{FMA2C}\label{sec:FMA2C}

FMA2C\cite{chu2019multi} is an advanced approach to traffic signal control that utilizes a hierarchical framework to optimize traffic flow in urban environments. It builds upon the prior work of MA2C (Multi-Agent Advantage Actor-Critic) by introducing managing agents to coordinate and oversee workers responsible for signal control at intersections.

\subsection{Basic Concepts}

\subsubsection{Workers (Intersection-Level Agents)}

In FMA2C, the core agents responsible for signal control at intersections are called workers. Each worker operates independently as an advantage actor-critic agent. The workers are tasked with making real-time decisions regarding traffic signal timings at their respective intersections.

\subsubsection{Managing Agents (Region-Level Agents)}

In FMA2C, managing agents are introduced, and in comparison to employees, they function at a higher level of the organizational structure. Within the traffic network, the responsibility for a particular region or area falls on the shoulders of each managing agent. These management agents are in charge of many personnel and are tasked with increasing the efficiency of traffic flow within the regions that they are responsible for.

\subsection{Hierarchical Reinforcement Learning}

FMA2C leverages hierarchical reinforcement learning to improve traffic signal coordination. The hierarchy involves two levels: managing agents at the top level and workers at the lower level.

\subsubsection{Managing Agent Training}

Training is provided to managing agents so that they can maximize the flow of traffic within the regions to which they are allocated. They are given high-level aims and objectives relating to traffic, such as reducing congestion as much as possible or increasing the amount of traffic that can pass through an area. The governing agents will base their judgments at the regional level on these aims.

The training of managing agents can be formulated as a reinforcement learning problem, where the managing agent learns a policy \(\pi_m\) to maximize a region-specific objective function:

\[
J_m(\pi_m) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R_t^m\right]
\]

Where:
\begin{align*}
    &J_m(\pi_m) \text{ - Expected cumulative reward for managing agent } m\\
    &\pi_m \text{ - Policy of managing agent } m\\
    &\gamma \text{ - Discount factor}\\
    &R_t^m \text{ - Region-specific reward at time step } t
\end{align*}

\subsubsection{Worker Training}

Workers, on the other hand, are trained to incorporate the high-level goals set by their respective managing agents into their local decision-making process. This hierarchical training ensures that workers align their actions with the broader objectives of traffic flow optimization.

The training of workers also involves reinforcement learning, where each worker learns a policy \(\pi_w\) to maximize its intersection-specific objective function:

\[
J_w(\pi_w) = \mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^t R_t^w\right]
\]

Where:
\begin{align*}
    &J_w(\pi_w) \text{ - Expected cumulative reward for worker } w\\
    &\pi_w \text{ - Policy of worker } w\\
    &\gamma \text{ - Discount factor}\\
    &R_t^w \text{ - Intersection-specific reward at time step } t
\end{align*}

\subsection{Coordination Mechanisms}

In order to maintain efficient traffic signal control, FMA2C makes use of a variety of procedures that facilitate collaboration between managing agents and personnel. The transmission of high-level goals, the distribution of rewards, and coordination through a central mechanism are some examples of the methods that may be used.

\subsection{Performance Improvement}

Through the implementation of a hierarchical structure that enables coordinated decision-making at both the regional and junction levels, FMA2C intends to accomplish the goals of enhancing traffic flow while simultaneously decreasing congestion. FMA2C's purpose is to optimize the timing of traffic signals in an effective manner by ensuring that the activities of workers are aligned with the goals of management agents.


